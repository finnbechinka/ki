{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZvfzID2ROXj"
      },
      "source": [
        "# Multilayer Perceptron (MLP)\n",
        "\n",
        "Author: canan.yildiz@tau.edu.tr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_9ZljWLROXk"
      },
      "source": [
        "## 1 - Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xV6IT6HKROXl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import scipy\n",
        "import sklearn.datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from scipy.special import expit     # expit is the sigmoid function\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "np.random.seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 - Activation Functions"
      ],
      "metadata": {
        "id": "h9Ak6KPWPAKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    \"\"\"\n",
        "    Implements the sigmoid activation in numpy \n",
        "    Arguments:\n",
        "      Z -- numpy array of any shape\n",
        "    Returns:\n",
        "      A -- output of sigmoid(z), same shape as Z\n",
        "    \"\"\"\n",
        "    A = 1/(1+np.exp(-Z))\n",
        "    \n",
        "    return A\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"\n",
        "    Implement the RELU function.\n",
        "    Arguments:\n",
        "      Z -- Output of the linear layer, of any shape\n",
        "    Returns:\n",
        "      A -- Post-activation parameter, of the same shape as Z\n",
        "    \"\"\"\n",
        "    A = np.maximum(0,Z)\n",
        "    assert(A.shape == Z.shape)\n",
        "\n",
        "    return A"
      ],
      "metadata": {
        "id": "DRJrvGwCPFwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To be used in the backpropagation step\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      dA --- gradients for the activations of the current layer l: any shape\n",
        "      Z --- input of the relu(Z) during the forward propagation step of the current layer l\n",
        "    Returns:\n",
        "      dZ --- gradients for Z (Partial derivative of the cost with respect to Z)\n",
        "    \"\"\"\n",
        "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
        "    dZ[Z <= 0] = 0                # When z <= 0, you should set dz to 0. \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      dA --- gradients for the activations of the current layer l: any shape\n",
        "      Z --- input of the relu(Z) during the forward propagation step of the current layer l\n",
        "    Returns:\n",
        "      dZ -- gradients for Z (Partial derivative of the cost with respect to Z)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n",
        "\n",
        "    ### Q : How does the notation exactly work: dZ[Z <= 0] = 0  ?"
      ],
      "metadata": {
        "id": "YOTedDdkeNWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_costs(costs, learning_rate):\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "99r7HLyE5dtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cm-mLsvXHIn"
      },
      "source": [
        "## 3 - Load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opW6cJdeHAGV"
      },
      "outputs": [],
      "source": [
        "# Generate data that is not linearly separable\n",
        "np.random.seed(7)\n",
        "X1_raw, Y1_raw = sklearn.datasets.make_moons(n_samples=100, \n",
        "                        shuffle=True, noise=0.1, random_state=None)\n",
        "\n",
        "# Tip: You can also use other datasets, e.g. sklearn.datasets.make_circles\n",
        "# Note that the \"test your implementation\" results in this notebook are calculated using the above dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the data:\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X1_raw[:, 0], X1_raw[:, 1], marker='o', c=Y1_raw, s=25, edgecolor='k')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "H_jp641ZHm2m",
        "outputId": "da52b5da-58af-404c-e921-a92d9817342a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and adjust dimensions if necessary\n",
        "print(X1_raw.shape)\n",
        "print(Y1_raw.shape)\n",
        "\n",
        "# We want to have one datapoint per column in X: (2, 100)\n",
        "# and one label per column in Y : (1, 100)\n",
        "X1 = np.transpose(X1_raw)\n",
        "Y1 = Y1_raw.reshape(1, Y1_raw.shape[0])\n",
        "print(X1.shape)\n",
        "print(Y1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYzkfzpbIqAI",
        "outputId": "35625a13-967e-4f37-f91b-74ee2a15da49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 2)\n",
            "(100,)\n",
            "(2, 100)\n",
            "(1, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmayLUFQROXm"
      },
      "source": [
        "## 4 - Initialization of weights and biases\n",
        "\n",
        "Implement the parameter initialization for one layer of a neural network.\n",
        "\n",
        "- Use random initialization for the weight matrices. You can use `np.random.randn(shape)*0.01` with the correct shape.\n",
        "- Use zero initialization for the biases. You can use `np.zeros(shape)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UnGADNdCROXm"
      },
      "outputs": [],
      "source": [
        "def init_params(n_prev, n_curr):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    n_prev --- size of the previous layer (size of the input of the current layer)\n",
        "    n_curr --- size of the current layer\n",
        "    \n",
        "    Returns:\n",
        "    parameters --- (W, b): weight matrix W of shape (n_curr, n_prev), bias vector b of shape (n_curr, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    W = None\n",
        "    b = None\n",
        "    \n",
        "    return (W, b)   \n",
        "\n",
        "### Q : How to initialize random parameters? Why multiply with 0.01?\n",
        "### Q : Why is it not a good idea to use zero initialization for the weight matrices? (Hint: symmetry braking) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQhG4GKSY5I2"
      },
      "source": [
        "### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-WjXB2LROXn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "(W,b) = init_params(3,1)\n",
        "print(\"W = \" + str(W))\n",
        "print(\"b = \" + str(b))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code. The expected output is:\n",
        "```\n",
        "W = [[ 0.00496714 -0.00138264  0.00647689]]\n",
        "b = [[0.]]\n",
        "```"
      ],
      "metadata": {
        "id": "Etz4aiMUUhKF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHOQU5yVROXr"
      },
      "source": [
        "## 5 - Forward propagation step\n",
        "Implement the forward propagation step for a given layer $l$ of a neural network. The activation \"g\" can be sigmoid() or relu(). $A^{[l-1]}$ is the output of the previous layer (note that $A^{[0]}$ is the input X). You may find `np.dot()` useful.\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]} \\tag{1}$$\n",
        "\n",
        "$$A^{[l]} = g(Z^{[l]}) \\tag{2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hnOC5ZDdROX4"
      },
      "outputs": [],
      "source": [
        "def forward_step(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    A_prev --- activations of previous layer: numpy array of shape (size of prev layer, number of examples)\n",
        "    W --- weight matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b --- bias vector: numpy array of shape (size of current layer, 1)\n",
        "    activation --- activation of current layer: \"sigmoid\" or \"relu\" (as string)\n",
        "\n",
        "    Returns:\n",
        "    (Z,A) --- logits and outputs of current layer: each of shape (size of current layer, number of examples)\n",
        "    \"\"\"\n",
        "    \n",
        "    # The linear part of a layer's forward propagation.\n",
        "    Z = None\n",
        "    assert(Z.shape == (W.shape[0], A_prev.shape[1]))\n",
        "\n",
        "    # The none-linear part\n",
        "    if activation == \"sigmoid\":\n",
        "        A = None\n",
        "    \n",
        "    elif activation == \"relu\":   \n",
        "        A = None\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "\n",
        "    return (Z,A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB7xY4pDZKs2"
      },
      "source": [
        "### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8haTPuzsROX5",
        "outputId": "9cf700bd-0f29-4101-dfd7-deac384b5f97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "With sigmoid: A = [[0.87368765 0.83243148]]\n",
            "With ReLU: A = [[1.93396509 1.6029586 ]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(42)\n",
        "A_prev = np.random.randn(3,2)\n",
        "W = np.random.randn(1,3)\n",
        "b = np.random.randn(1,1)\n",
        "\n",
        "(Z_s, A_s) = forward_step(A_prev, W, b, activation = \"sigmoid\")\n",
        "(Z_r, A_r) = forward_step(A_prev, W, b, activation = \"relu\")\n",
        "\n",
        "print(\"With sigmoid: A = \" + str(A_s))\n",
        "print(\"With ReLU: A = \" + str(A_r))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following code. The expected output is:\n",
        "```\n",
        "With sigmoid: A = [[0.87368765 0.83243148]]  \n",
        "With ReLU: A = [[1.93396509 1.6029586 ]]\n",
        "```\n"
      ],
      "metadata": {
        "id": "0pGpBcW_Qzh_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDXl8mNqROX8"
      },
      "source": [
        "## 6 - Cost function\n",
        "\n",
        "Compute the cross-entropy cost $J$ using the following formula. Here, $y^{(i)}$ is the label of the i-th data point and $\\hat{y}^{(i)}$ is the prediction of the neural network for the i-th data point. m is the total number of data points.\n",
        "\n",
        " $$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{(i)}\\right)) \\tag{3}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "34hdbcH2ROX8"
      },
      "outputs": [],
      "source": [
        "def compute_cost(Y_hat, Y):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    Y_hat --- predictions: vector of shape (1, number of examples)\n",
        "    Y --- labels: vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost --- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]  # number of examples\n",
        "\n",
        "    cost = None\n",
        "    \n",
        "    cost = np.squeeze(cost)      # turns [[17]] into 17\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODjzBS_PZhoc"
      },
      "source": [
        "### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIkSKYKlROX8",
        "outputId": "be173db1-b749-49d1-ec05-1b4abee15997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost = 0.41493159961539694\n"
          ]
        }
      ],
      "source": [
        "Y = np.asarray([[1, 1, 1]])\n",
        "Y_hat = np.array([[.8,.9,0.4]])\n",
        "\n",
        "print(\"cost = \" + str(compute_cost(Y_hat, Y)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDgzTUBAROX9"
      },
      "source": [
        "The expected output is:\n",
        "```\n",
        "0.41493159961539694\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fueKUh9oROX9"
      },
      "source": [
        "## 7 - Backward propagation step\n",
        "\n",
        "Implement the backpropagation step for one layer (forward_step $l$) of the neural network by calculating the following partial derivatives, which are the part of the \"chain\" belonging to this layer. $g(.)$ is the activation function in the current layer, $g'$ is `sigmoid_backward` or `relu_backward` accordingly: \n",
        "\n",
        "$$dZ^{[l]} := \\frac{\\partial \\mathcal{J} }{\\partial Z^{[l]}} = dA^{[l]} * g'(Z^{[l]}) \\tag{4}$$<br>\n",
        "\n",
        "$$ dW^{[l]} := \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{5}$$<br>\n",
        "\n",
        "$$ db^{[l]} := \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{6}$$<br>\n",
        "\n",
        "$$ dA^{[l-1]} := \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{7}$$<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cRUT2t_zROX-"
      },
      "outputs": [],
      "source": [
        "def backward_step(A_prev, W, b, a, Z, dA):\n",
        "    \"\"\"    \n",
        "    Arguments:\n",
        "    dA --- gradients for the activations of current layer l \n",
        "    cache --- (A_prev, W, b, Z) tuple stored in the forward_step of layer l\n",
        "    activation --- the activation function used in current layer l: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev --- Gradient for the activations of the previous layer l-1: same shape as A_prev\n",
        "    dW --- gradients for the weights W of current layer l: same shape as W\n",
        "    db --- gradients for the biases b of current layer l: same shape as b\n",
        "    \"\"\"\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    # The non-linear backward part\n",
        "    if a == \"relu\":\n",
        "        dZ = None\n",
        "        \n",
        "    elif a == \"sigmoid\":\n",
        "        dZ = None\n",
        "    \n",
        "\n",
        "    # The linear backward part\n",
        "    dW = None\n",
        "    db = None\n",
        "    dA_prev = None\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return (dA_prev, dW, db)\n",
        "\n",
        "### Q : Give a short explanation of what relu_backward is doing\n",
        "### Q : What happens when the \"axis\" parameter of np.sum is set to 1?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap78YUWHZ_hv"
      },
      "source": [
        "### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8UkkwEBROX_"
      },
      "outputs": [],
      "source": [
        "np.random.seed(7)\n",
        "dAL = np.random.randn(1,2)         # (size of this layer, number of examples)\n",
        "A_prev = np.random.randn(2,2)     # (size of prev layer, number of examples)\n",
        "W = np.random.randn(1,2)\n",
        "b = np.random.randn(1,1)\n",
        "Z = np.random.randn(1,2)\n",
        "\n",
        "(dA_prev, dW, db) = backward_step(A_prev, W, b, \"sigmoid\", Z, dAL)\n",
        "print (\"for sigmoid:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db) + \"\\n\")\n",
        "\n",
        "dA_prev, dW, db = backward_step(A_prev, W, b, \"relu\", Z, dAL)\n",
        "print (\"for relu:\")\n",
        "print (\"dA_prev = \"+ str(dA_prev))\n",
        "print (\"dW = \" + str(dW))\n",
        "print (\"db = \" + str(db))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Expected output is:\n",
        "```\n",
        "for sigmoid:\n",
        "dA_prev = [[-3.44320629e-04  9.41999464e-05]\n",
        " [-6.78568478e-01  1.85644161e-01]]\n",
        "dW = [[-0.01521102 -0.15265126]]\n",
        "db = [[0.14045634]]\n",
        "\n",
        "for relu:\n",
        "dA_prev = [[-1.50522018e-03  0.00000000e+00]\n",
        " [-2.96640654e+00  0.00000000e+00]]\n",
        "dW = [[ 0.02774167 -0.66684733]]\n",
        "db = [[0.84526285]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "b94fp74elEVE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o47XONbCROYA"
      },
      "source": [
        "## 8 - Parameter Updates\n",
        "Implement the function for the parameter update using the following formulas. $\\alpha$ is the learning rate.\n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{8}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{9}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "peuckqV0ROYB"
      },
      "outputs": [],
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    \"\"\"    \n",
        "    Arguments:\n",
        "    parameters --- (W,b,a): parameters of current layer l\n",
        "    grads --- (dW, db): gradients for the weights and biases of current layer l\n",
        "    \n",
        "    Returns:\n",
        "    (W_new, b_new) --- updated weights and biases\n",
        "    \"\"\"\n",
        "    (W,b,a) = parameters\n",
        "    (dW,db) = gradients\n",
        "    W_new = None\n",
        "    b_new = None\n",
        "\n",
        "    return (W_new, b_new, a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFf3Zt7fapah"
      },
      "source": [
        "### Test your implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWPMqeT3ROYC",
        "outputId": "edac82fa-32b6-4fde-f273-dc260517c0c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_new = [[ 1.5887599  -0.52598722  0.09536306]\n",
            " [ 0.42467111 -0.83945297  0.02820121]]\n",
            "b_new = [[ 0.02338452]\n",
            " [-1.60940017]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "W = np.random.randn(2,3)\n",
        "b = np.random.randn(2,1)\n",
        "a = \"relu\"\n",
        "dW = np.random.randn(2,3)\n",
        "db = np.random.randn(2,1)\n",
        "(W_new, b_new, a) = update_parameters((W,b,a), (dW,db), 0.1)\n",
        "\n",
        "print (\"W_new = \"+ str(W_new))\n",
        "print (\"b_new = \"+ str(b_new))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected output is:\n",
        "```\n",
        "W_new = [[ 1.5887599  -0.52598722  0.09536306]\n",
        " [ 0.42467111 -0.83945297  0.02820121]]\n",
        "b_new = [[ 0.02338452]\n",
        " [-1.60940017]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "K8RmmJVnnh5T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZbYcbanetEL"
      },
      "source": [
        "## 9 - Model Initialization\n",
        "\n",
        "Implement the initialization of a model with the given layer dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def init_model(layer_dims, activations):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims --- array of dimensions, for each layer including input layer (e.g. (2,3,1))\n",
        "    activations --- array of activation functıons to be used, for each layer except the input layer (e.g. (\"relu\", \"sigmoid\"))\n",
        "\n",
        "    Returns:\n",
        "    model --- array of (W, b, activation) tuples, one for each layer\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(layer_dims) - 1\n",
        "    model = [None for i in range (0,L+1,1)]\n",
        "    model[0]=(None,None,None)    # Interpreting Input layer as layer number 0 (no parameters, no activation)\n",
        "\n",
        "    for l in range(1,L+1,1):\n",
        "      (W, b) = None\n",
        "      model[l] = None\n",
        "    \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "SwMLW0FRu2xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your implementation"
      ],
      "metadata": {
        "id": "tms2QaOCHnDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(7)\n",
        "model = init_model((2,3,1), (\"relu\",\"sigmoid\"))\n",
        "for l in range(1,len(model),1):\n",
        "  (W,b,a) = model[l]\n",
        "  print(\"Layer\",l,\":\",W.shape[1],\"inputs and\", W.shape[0],\"outputs ( activation\",a,\")\")\n",
        "  print(\"W: \\n\", W)\n",
        "  print(\"b: \\n\", b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PD9GN6bqx761",
        "outputId": "9e5977e8-9c19-460e-cd80-48e66acf152c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 : 2 inputs and 3 outputs ( activation relu )\n",
            "W: \n",
            " [[ 1.69052570e-02 -4.65937371e-03]\n",
            " [ 3.28201637e-04  4.07516283e-03]\n",
            " [-7.88923029e-03  2.06557291e-05]]\n",
            "b: \n",
            " [[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "Layer 2 : 3 inputs and 1 outputs ( activation sigmoid )\n",
            "W: \n",
            " [[-8.90385858e-06 -1.75472431e-02  1.01765801e-02]]\n",
            "b: \n",
            " [[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output is:\n",
        "```\n",
        "Layer 1 : 2 inputs and 3 outputs (activation relu )\n",
        "W: \n",
        " [[ 1.69052570e-02 -4.65937371e-03]\n",
        " [ 3.28201637e-04  4.07516283e-03]\n",
        " [-7.88923029e-03  2.06557291e-05]]\n",
        "b: \n",
        " [[0.]\n",
        " [0.]\n",
        " [0.]]\n",
        "Layer 2 : 3 inputs and 1 outputs (activation sigmoid )\n",
        "W: \n",
        " [[-8.90385858e-06 -1.75472431e-02  1.01765801e-02]]\n",
        "b: \n",
        " [[0.]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DgOVcG5tH1WN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmiJT1_WkgIc"
      },
      "source": [
        "## 10 - Model Trainig\n",
        "\n",
        "Implement the function for training a given model on a given dataset.\n",
        "\n",
        "Log the cost regularly to see if your model is learning correctly. The cost should be decreasing. After the training, you will have your neural network model with the learned **parameters**, which corresponds to your hypothesis function $h^{*}(x)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWdOQexofFvJ"
      },
      "outputs": [],
      "source": [
        "def train_model(model, X, Y, num_iterations = 3000, learning_rate = 0.01, print_cost=False):\n",
        "    \"\"\"\n",
        "\n",
        "    Arguments:\n",
        "    model --- model to be trained: array of (W, b, activation) tuples, one for each layer\n",
        "    X --- input data: of shape (n, number of examples)\n",
        "    Y --- labels for the input data: of shape (1, number of examples)\n",
        "    num_iterations --- number of iterations of the optimization loop\n",
        "    learning_rate --- learning rate of the gradient descent update rule\n",
        "    print_cost --- If set to True, this will print the cost every 100 iterations \n",
        "    \n",
        "    Returns:\n",
        "    model --- model with trained parameters\n",
        "    costs --- costs saved every 100th itertion during training\n",
        "    \"\"\"\n",
        "\n",
        "    n = X.shape[0]                        # number of features\n",
        "    m = X.shape[1]                        # number of examples\n",
        "    L = len(model) - 1                    # number of layers in model (input layer not included)\n",
        "\n",
        "    assert (n == model[1][0].shape[1])    # n == (layer 1 -> W -> number of columns) ?\n",
        "    \n",
        "    costs = []                                # to keep track of the cost\n",
        "    grads = [None for i in range(0,L+1,1)]\n",
        "    grads[0] = (None, None)                   # no gradients for input layer 0\n",
        "    outputs = [None for i in range(0,L+1,1)]\n",
        "    outputs[0] = (None,X)                     # \"Outputs\" of input layer are the datapoints X\n",
        "\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation\n",
        "        A_prev = None\n",
        "        for l in range(1,L+1,1):\n",
        "          (W,b,a) = None\n",
        "          (Z,A) = None\n",
        "          outputs[l] = None\n",
        "          A_prev = None\n",
        "        AL = None          # Output of last layer L (i.e. predictions)\n",
        "\n",
        "        # Compute cost\n",
        "        cost = None\n",
        "        \n",
        "        # Initialize backward propagation by calculating gradients for AL\n",
        "        dAL = None\n",
        "        \n",
        "        # Backward propagation\n",
        "        dA = dAL\n",
        "        for l in range(L,0,-1):\n",
        "          (W,b,a) = None\n",
        "          (Z_prev,A_prev) = None\n",
        "          (Z,A) = None\n",
        "          (dA_prev, dW, db) = None\n",
        "          grads[l] = None\n",
        "          dA = None\n",
        "        \n",
        "        # Update model parameters\n",
        "        for l in range(1,L+1,1):\n",
        "          model[l] = None\n",
        "        \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "    \n",
        "    \n",
        "    return model, costs\n",
        "\n",
        "    ### Q : What does np.divide do exactly?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your implementation"
      ],
      "metadata": {
        "id": "BRD74_pFdsIV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZOhCR96pMmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaf5e07-249c-4406-dc7a-18bbd7a26a35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1\n",
            "W: \n",
            " [[ 5.79827820e-01 -1.24586352e+00]\n",
            " [-1.38621994e-01  1.62132531e+00]\n",
            " [-3.92030584e-03 -1.59684129e-03]]\n",
            "b: \n",
            " [[ 1.27401504]\n",
            " [ 0.90924215]\n",
            " [-0.00462265]]\n",
            "Layer 2\n",
            "W: \n",
            " [[ 1.8734467  -1.86390208  0.00894749]]\n",
            "b: \n",
            " [[-0.15249612]]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "model = init_model((2,3,1), (\"relu\",\"sigmoid\"))\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.1\n",
        "print_cost=False\n",
        "model, costs = train_model(model, X1, Y1, num_iterations, learning_rate, print_cost)\n",
        "#plot_costs(costs, learning_rate)\n",
        "for l in range(1,len(model),1):\n",
        "  (W,b,a) = model[l]\n",
        "  print(\"Layer\",l)\n",
        "  print(\"W: \\n\", W)\n",
        "  print(\"b: \\n\", b)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output is:\n",
        "\n",
        "```\n",
        "Layer 1\n",
        "W: \n",
        " [[ 5.79827820e-01 -1.24586352e+00]\n",
        " [-1.38621994e-01  1.62132531e+00]\n",
        " [-3.92030584e-03 -1.59684129e-03]]\n",
        "b: \n",
        " [[ 1.27401504]\n",
        " [ 0.90924215]\n",
        " [-0.00462265]]\n",
        "Layer 2\n",
        "W: \n",
        " [[ 1.8734467  -1.86390208  0.00894749]]\n",
        "b: \n",
        " [[-0.15249612]]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "eiymIiVPm4PV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEEEaqvslaGO"
      },
      "source": [
        "## 11 - Making Predictions\n",
        "\n",
        "Using the learned parameters you can now predict the output for new input values $X$.\n",
        "\n",
        "Implement a function (`model_forward(X, Y, parameters)`) that takes input values $X$ and computes the corresponding output values (predictions) using the learned parameters.\n",
        "\n",
        "(to  be continued...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-YsJC3UzpMj"
      },
      "outputs": [],
      "source": [
        "def model_forward(model, X):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the given neural network model\n",
        "    \n",
        "    Arguments:\n",
        "    model --- trained model\n",
        "    X --- data: numpy array of shape (input size, number of examples)\n",
        "    \n",
        "    Returns:\n",
        "    AL --- putput of last layer (predictions)\n",
        "    outputs --- list of calculated outputs during forward_step\n",
        "    \"\"\"\n",
        "\n",
        "    L = len(model) - 1                        # number of layers in the neural network (input layer not included)\n",
        "    outputs = [None for i in range(0,L+1,1)]\n",
        "    outputs[0] = (None,X)                     # \"Outputs\" of input layer are the datapoints X\n",
        "\n",
        "    # Forward propagation\n",
        "    A_prev = X\n",
        "    for l in range(1,L+1,1):\n",
        "      (W,b,a) = None\n",
        "      (Z,A) = None\n",
        "      outputs[l] = None\n",
        "      A_prev = None\n",
        "    AL = None        # Output of last layer L (i.e. predictions)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test your implementation"
      ],
      "metadata": {
        "id": "3unnp6WbqPj1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89MPrWYL2VQM",
        "outputId": "583fb2f4-fb5b-4f7b-8f6a-3b2cb9d995c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01030258 0.99269639 0.94674441 0.41083869 0.05092415 0.96912069\n",
            " 0.97984208 0.94746217 0.00974002]\n",
            "[0 1 1 0 0 1 1 1 0]\n"
          ]
        }
      ],
      "source": [
        "np.random.seed(7)\n",
        "model = init_model((2,3,1), (\"relu\",\"sigmoid\"))\n",
        "num_iterations = 1000\n",
        "learning_rate = 0.1\n",
        "print_cost=False\n",
        "trained_model, costs = train_model(model, X1, Y1, num_iterations, learning_rate, print_cost)\n",
        "probabilities, outputs = model_forward(model, X1)\n",
        "print(probabilities[0,1:10])\n",
        "print(Y1[0,1:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected output is:\n",
        "\n",
        "```\n",
        "[0.01030258 0.99269639 0.94674441 0.41083869 0.05092415 0.96912069\n",
        " 0.97984208 0.94746217 0.00974002]\n",
        "[0 1 1 0 0 1 1 1 0]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ct4BTnE9qSnK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAHLexjN5oEZ"
      },
      "source": [
        "## 12 - Performance Analysis\n",
        "\n",
        "Implement a function for calculating the accuracy of given probability outputs, given the expetcted labels Y."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_accuracy(AL, Y):\n",
        "    \"\"\"    \n",
        "    Arguments:\n",
        "    AL --- probability outputs to be assessed\n",
        "    Y --- ground truth (expected true labels)\n",
        "    \n",
        "    Returns:\n",
        "    p --- predicted classes for the given dataset X\n",
        "    accuracy --- accuracy of the predictions (correct rate)\n",
        "    \"\"\"\n",
        "    m = Y.shape[1]    # number of data points\n",
        "\n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, AL.shape[1]):\n",
        "        if AL[0,i] > 0.5:\n",
        "            p[0,i] = 1\n",
        "        else:\n",
        "            p[0,i] = 0\n",
        "    \n",
        "    accuracy = np.sum((p == Y)/m)\n",
        "        \n",
        "    return (p, accuracy)"
      ],
      "metadata": {
        "id": "fDk0R5Pf3ihh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13 - Test your implementation"
      ],
      "metadata": {
        "id": "eoxuTyTE48M8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hYf0KbjAX-B"
      },
      "outputs": [],
      "source": [
        "# Train set accuray of our model_1\n",
        "np.random.seed(77)\n",
        "model = init_model((2,5,1), (\"relu\",\"sigmoid\"))\n",
        "num_iterations = 2000\n",
        "learning_rate = 0.1\n",
        "print_cost=True\n",
        "traıned_model, costs = train_model(model, X1, Y1, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "AY, outputs = model_forward(trained_model, X1)\n",
        "\n",
        "plot_costs(costs, learning_rate)\n",
        "\n",
        "p, accuracy = model_accuracy(AY, Y1)\n",
        "print(\"Train Accuracy: \", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XRo5IOBE5Jmq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9Cm-mLsvXHIn",
        "YQhG4GKSY5I2",
        "vB7xY4pDZKs2",
        "ODjzBS_PZhoc",
        "Ap78YUWHZ_hv",
        "VFf3Zt7fapah",
        "tms2QaOCHnDn",
        "BRD74_pFdsIV",
        "3unnp6WbqPj1"
      ],
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "c4HO0",
      "launcher_item_id": "lSYZM"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}